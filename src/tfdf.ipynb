{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# Download the dataset\n",
    "# !wget -q https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins.csv -O /tmp/penguins.csv\n",
    "\n",
    "# Load a dataset into a Pandas Dataframe.\n",
    "dataset_df = pd.read_csv(\"/home/huigang39/e-skin/data/train.csv\")\n",
    "\n",
    "# Display the first 3 examples.\n",
    "dataset_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"label\"\n",
    "\n",
    "classes = dataset_df[label].unique().tolist()\n",
    "print(f\"Label classes: {classes}\")\n",
    "\n",
    "dataset_df[label] = dataset_df[label].map(classes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_ratio=0.30):\n",
    "  \"\"\"Splits a panda dataframe in two.\"\"\"\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  return dataset[~test_indices], dataset[test_indices]\n",
    "\n",
    "\n",
    "train_ds_pd, test_ds_pd = split_dataset(dataset_df)\n",
    "print(\"{} examples in training, {} examples for testing.\".format(\n",
    "    len(train_ds_pd), len(test_ds_pd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建随机森林\n",
    "model = tfdf.keras.RandomForestModel()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(x=train_ds)\n",
    "\n",
    "# 评估模型\n",
    "model.compile(metrics=[\"accuracy\"])\n",
    "evaluation = model.evaluate(test_ds, return_dict=True)\n",
    "print()\n",
    "\n",
    "for name, value in evaluation.items():\n",
    "  print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"plot.html\", \"w\") as f:\n",
    "  f.write(tfdf.model_plotter.plot_model(model, tree_idx=0, max_depth=3))\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='./plot.html', width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logs = model.make_inspector().training_logs()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Accuracy (out-of-bag)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Logloss (out-of-bag)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测标签\n",
    "data = pd.read_csv(\"/home/huigang39/e-skin/data/test.csv\")\n",
    "data = tfdf.keras.pd_dataframe_to_tf_dataset(data)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 假设predictions是模型预测的概率分布\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# 使用numpy的argmax函数找到最大概率对应的索引\n",
    "predicted_label_index = np.argmax(predictions)\n",
    "\n",
    "print(\"预测标签:\", predicted_label_index)\n",
    "\n",
    "# 标签到类名的映射\n",
    "label_to_class = {index: class_name for index, class_name in enumerate(classes)}\n",
    "\n",
    "# 使用预测的索引获取对应的类名\n",
    "predicted_class = label_to_class[predicted_label_index]\n",
    "\n",
    "print(\"预测分类:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save(\"/home/huigang39/e-skin/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as call_get_leaves, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_6n30ir6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_6n30ir6/assets\n",
      "[INFO 23-06-01 09:14:59.2651 CST kernel.cc:1242] Loading model from path /tmp/tmp_6n30ir6/assets/ with prefix abb72d17f5864b67\n",
      "[INFO 23-06-01 09:14:59.5810 CST decision_forest.cc:660] Model loaded with 300 root(s), 109944 node(s), and 10 input feature(s).\n",
      "[INFO 23-06-01 09:14:59.5810 CST abstract_model.cc:1311] Engine \"RandomForestGeneric\" built\n",
      "[INFO 23-06-01 09:14:59.5810 CST kernel.cc:1074] Use fast generic engine\n",
      "2023-06-01 09:14:59.623681: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-06-01 09:14:59.623746: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-06-01 09:14:59.624078: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp_6n30ir6\n",
      "2023-06-01 09:14:59.629722: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-06-01 09:14:59.629767: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp_6n30ir6\n",
      "2023-06-01 09:14:59.645973: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-06-01 09:14:59.675901: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp_6n30ir6\n",
      "[INFO 23-06-01 09:14:59.6933 CST kernel.cc:1242] Loading model from path /tmp/tmp_6n30ir6/assets/ with prefix abb72d17f5864b67\n",
      "[INFO 23-06-01 09:15:00.1234 CST decision_forest.cc:660] Model loaded with 300 root(s), 109944 node(s), and 10 input feature(s).\n",
      "[INFO 23-06-01 09:15:00.1234 CST kernel.cc:1074] Use fast generic engine\n",
      "2023-06-01 09:15:00.140669: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 516591 microseconds.\n",
      "loc(fused[\"SimpleMLCreateModelResource:\", \"SimpleMLCreateModelResource\"]): error: 'tf.SimpleMLCreateModelResource' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(callsite(fused[\"SimpleMLInferenceOpWithHandle:\", callsite(\"inference_op@__inference_call_880\"(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py\":707:0) at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py\":795:0 at callsite(\"<string>\":525:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/util/tf_export.py\":413:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow_decision_forests/tensorflow/ops/inference/api.py\":376:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow_decision_forests/keras/core_inference.py\":648:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py\":468:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py\":502:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py\":451:0 at \"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow_decision_forests/keras/core_inference.py\":647:0)))))))))] at fused[\"StatefulPartitionedCall:\", callsite(\"random_forest_model/StatefulPartitionedCall@__inference__wrapped_model_3318\"(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/save.py\":1276:0) at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/save.py\":1240:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":1283:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\":205:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":1353:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":1373:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":940:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":962:0 at callsite(\"/tmp/ipykernel_4929/2619304295.py\":3:0 at \"/home/huigang39/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\":3508:0)))))))))]) at fused[\"StatefulPartitionedCall:\", callsite(\"StatefulPartitionedCall@__inference_signature_wrapper_3536\"(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/save.py\":1276:0) at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/save.py\":1240:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":1283:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\":205:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":1353:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":1373:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":940:0 at callsite(\"/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\":962:0 at callsite(\"/tmp/ipykernel_4929/2619304295.py\":3:0 at \"/home/huigang39/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\":3508:0)))))))))]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): error: 'tf.SimpleMLInferenceOpWithHandle' op is neither a custom op nor a flex op\n",
      "2023-06-01 09:15:00.210186: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2040] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:\n",
      "Resource ops: SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandle\n",
      "Details:\n",
      "\ttf.SimpleMLCreateModelResource() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", shared_name = \"simple_ml_model_d6f82eea-0ba4-4d85-a8c3-b1dcb7760c1a\"}\n",
      "\ttf.SimpleMLInferenceOpWithHandle(tensor<?x10xf32>, tensor<0x0xf32>, tensor<0x0xi32>, tensor<0xi32>, tensor<1xi64>, tensor<1xi64>, tensor<!tf_type.resource>) -> (tensor<?x3xf32>, tensor<3x!tf_type.string>) : {dense_output_dim = 3 : i64, device = \"\"}\n",
      "error: failed while converting: 'main': \n",
      "Some ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom \n",
      "Custom ops: SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandle\n",
      "Details:\n",
      "\ttf.SimpleMLCreateModelResource() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", shared_name = \"simple_ml_model_d6f82eea-0ba4-4d85-a8c3-b1dcb7760c1a\"}\n",
      "\ttf.SimpleMLInferenceOpWithHandle(tensor<?x10xf32>, tensor<0x0xf32>, tensor<0x0xi32>, tensor<0xi32>, tensor<1xi64>, tensor<1xi64>, tensor<!tf_type.resource>) -> (tensor<?x3xf32>, tensor<3x!tf_type.string>) : {dense_output_dim = 3 : i64, device = \"\"}\n",
      "\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "<unknown>:0: error: loc(fused[\"SimpleMLCreateModelResource:\", \"SimpleMLCreateModelResource\"]): 'tf.SimpleMLCreateModelResource' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"SimpleMLCreateModelResource:\", \"SimpleMLCreateModelResource\"]): Error code: ERROR_NEEDS_CUSTOM_OPS\n/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:707:0: error: 'tf.SimpleMLInferenceOpWithHandle' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:707:0: note: Error code: ERROR_NEEDS_CUSTOM_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom \nCustom ops: SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandle\nDetails:\n\ttf.SimpleMLCreateModelResource() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", shared_name = \"simple_ml_model_d6f82eea-0ba4-4d85-a8c3-b1dcb7760c1a\"}\n\ttf.SimpleMLInferenceOpWithHandle(tensor<?x10xf32>, tensor<0x0xf32>, tensor<0x0xi32>, tensor<0xi32>, tensor<1xi64>, tensor<1xi64>, tensor<!tf_type.resource>) -> (tensor<?x3xf32>, tensor<3x!tf_type.string>) : {dense_output_dim = 3 : i64, device = \"\"}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 转换模型为TensorFlow Lite格式\u001b[39;00m\n\u001b[1;32m      2\u001b[0m converter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mTFLiteConverter\u001b[39m.\u001b[39mfrom_keras_model(model)\n\u001b[0;32m----> 3\u001b[0m tflite_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n\u001b[1;32m      5\u001b[0m \u001b[39m# 保存转换后的模型\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconverted_model.tflite\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:962\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(convert_func)\n\u001b[1;32m    960\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    961\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 962\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_and_export_metrics(convert_func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:940\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m    939\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m--> 940\u001b[0m result \u001b[39m=\u001b[39m convert_func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    941\u001b[0m elapsed_time_ms \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mprocess_time() \u001b[39m-\u001b[39m start_time) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m    942\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1373\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[39m@_export_metrics\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1362\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m \n\u001b[1;32m   1364\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[39m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1373\u001b[0m   saved_model_convert_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_as_saved_model()\n\u001b[1;32m   1374\u001b[0m   \u001b[39mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1375\u001b[0m     \u001b[39mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1356\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m   graph_def, input_tensors, output_tensors \u001b[39m=\u001b[39m (\n\u001b[1;32m   1353\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_keras_to_saved_model(temp_dir))\n\u001b[1;32m   1354\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaved_model_dir:\n\u001b[1;32m   1355\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(TFLiteKerasModelConverterV2,\n\u001b[0;32m-> 1356\u001b[0m                  \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mconvert(graph_def, input_tensors, output_tensors)\n\u001b[1;32m   1357\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1358\u001b[0m   shutil\u001b[39m.\u001b[39mrmtree(temp_dir, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1166\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1161\u001b[0m   logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mUsing new converter: If you encounter a problem \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1162\u001b[0m                \u001b[39m\"\u001b[39m\u001b[39mplease file a bug. You can opt-out \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1163\u001b[0m                \u001b[39m\"\u001b[39m\u001b[39mby setting experimental_new_converter=False\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1165\u001b[0m \u001b[39m# Converts model.\u001b[39;00m\n\u001b[0;32m-> 1166\u001b[0m result \u001b[39m=\u001b[39m _convert_graphdef(\n\u001b[1;32m   1167\u001b[0m     input_data\u001b[39m=\u001b[39;49mgraph_def,\n\u001b[1;32m   1168\u001b[0m     input_tensors\u001b[39m=\u001b[39;49minput_tensors,\n\u001b[1;32m   1169\u001b[0m     output_tensors\u001b[39m=\u001b[39;49moutput_tensors,\n\u001b[1;32m   1170\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconverter_kwargs)\n\u001b[1;32m   1172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimize_tflite_model(\n\u001b[1;32m   1173\u001b[0m     result, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_quant_mode, quant_io\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_new_quantizer)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:212\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     report_error_message(\u001b[39mstr\u001b[39m(converter_error))\n\u001b[0;32m--> 212\u001b[0m   \u001b[39mraise\u001b[39;00m converter_error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# Re-throws the exception.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    214\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:817\u001b[0m, in \u001b[0;36mconvert_graphdef\u001b[0;34m(input_data, input_tensors, output_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     model_flags\u001b[39m.\u001b[39moutput_arrays\u001b[39m.\u001b[39mappend(util\u001b[39m.\u001b[39mget_tensor_name(output_tensor))\n\u001b[0;32m--> 817\u001b[0m data \u001b[39m=\u001b[39m convert(\n\u001b[1;32m    818\u001b[0m     model_flags\u001b[39m.\u001b[39;49mSerializeToString(),\n\u001b[1;32m    819\u001b[0m     conversion_flags\u001b[39m.\u001b[39;49mSerializeToString(),\n\u001b[1;32m    820\u001b[0m     input_data\u001b[39m.\u001b[39;49mSerializeToString(),\n\u001b[1;32m    821\u001b[0m     debug_info_str\u001b[39m=\u001b[39;49mdebug_info\u001b[39m.\u001b[39;49mSerializeToString() \u001b[39mif\u001b[39;49;00m debug_info \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    822\u001b[0m     enable_mlir_converter\u001b[39m=\u001b[39;49menable_mlir_converter)\n\u001b[1;32m    823\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:322\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mfor\u001b[39;00m error_data \u001b[39min\u001b[39;00m _metrics_wrapper\u001b[39m.\u001b[39mretrieve_collected_errors():\n\u001b[1;32m    321\u001b[0m       converter_error\u001b[39m.\u001b[39mappend_error(error_data)\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mraise\u001b[39;00m converter_error\n\u001b[1;32m    324\u001b[0m \u001b[39mreturn\u001b[39;00m _run_deprecated_conversion_binary(model_flags_str,\n\u001b[1;32m    325\u001b[0m                                          conversion_flags_str, input_data_str,\n\u001b[1;32m    326\u001b[0m                                          debug_info_str)\n",
      "\u001b[0;31mConverterError\u001b[0m: <unknown>:0: error: loc(fused[\"SimpleMLCreateModelResource:\", \"SimpleMLCreateModelResource\"]): 'tf.SimpleMLCreateModelResource' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"SimpleMLCreateModelResource:\", \"SimpleMLCreateModelResource\"]): Error code: ERROR_NEEDS_CUSTOM_OPS\n/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:707:0: error: 'tf.SimpleMLInferenceOpWithHandle' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n/home/huigang39/.local/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:707:0: note: Error code: ERROR_NEEDS_CUSTOM_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom \nCustom ops: SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandle\nDetails:\n\ttf.SimpleMLCreateModelResource() -> (tensor<!tf_type.resource>) : {container = \"\", device = \"\", shared_name = \"simple_ml_model_d6f82eea-0ba4-4d85-a8c3-b1dcb7760c1a\"}\n\ttf.SimpleMLInferenceOpWithHandle(tensor<?x10xf32>, tensor<0x0xf32>, tensor<0x0xi32>, tensor<0xi32>, tensor<1xi64>, tensor<1xi64>, tensor<!tf_type.resource>) -> (tensor<?x3xf32>, tensor<3x!tf_type.string>) : {dense_output_dim = 3 : i64, device = \"\"}\n\n"
     ]
    }
   ],
   "source": [
    "# 转换模型为TensorFlow Lite格式\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 保存转换后的模型\n",
    "with open(\"converted_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
